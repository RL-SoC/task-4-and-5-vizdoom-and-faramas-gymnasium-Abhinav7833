# Required Imports
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from vizdoom import DoomGame, Mode, ScreenResolution
from gym.spaces import Discrete, Box
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.evaluation import evaluate_policy
from matplotlib import pyplot as plt
import os
import cv2

# Environment Wrapper
class VizDoomGym(Env):
    def __init__(self, config, render=False):
        super(VizDoomGym, self).__init__()
        self.game = DoomGame()
        self.game.load_config(config)
        self.game.set_screen_resolution(ScreenResolution.RES_640X480)
        self.game.set_window_visible(render)
        self.game.init()
        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)
        self.action_space = Discrete(self.game.get_available_buttons_size())

    def step(self, action):
        reward = self.game.make_action(action, 4)
        done = self.game.is_episode_finished()
        if not done:
            state = self.game.get_state().screen_buffer
            state = self.grayscale(state)
            info = {"ammo": self.game.get_state().game_variables[0]}
        else:
            state = np.zeros(self.observation_space.shape)
            info = {"ammo": 0}
        return state, reward, done, info

    def reset(self):
        self.game.new_episode()
        state = self.game.get_state().screen_buffer
        return self.grayscale(state)

    def close(self):
        self.game.close()

    def render(self, mode='human'):
        pass

    def grayscale(self, observation):
        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)
        resized = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_CUBIC)
        state = np.reshape(resized, (100, 160, 1))
        return state

# Custom Neural Network Architecture
class CustomCnnPolicy(nn.Module):
    def __init__(self, num_actions):
        super(CustomCnnPolicy, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.fc1 = nn.Linear(64 * 9 * 9, 512)
        self.fc2 = nn.Linear(512, num_actions)
        self.value_head = nn.Linear(512, 1)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = torch.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        logits = self.fc2(x)
        value = self.value_head(x)
        return logits, value

# Training and Logging Callback
class TrainAndLoggingCallback(BaseCallback):
    def __init__(self, check_freq, save_path, verbose=1):
        super(TrainAndLoggingCallback, self).__init__(verbose)
        self.check_freq = check_freq
        self.save_path = save_path

    def _init_callback(self):
        if self.save_path is not None:
            os.makedirs(self.save_path, exist_ok=True)

    def _on_step(self):
        if self.n_calls % self.check_freq == 0:
            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))
            self.model.save(model_path)
        return True

# Setup
CHECKPOINT_DIR = './train/train_basic'
LOG_DIR = './logs/log_basic'
callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)
env = VizDoomGym(config='github/VizDoom/scenarios/basic.cfg')
check_env(env)

# Training the Model
model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.0001, n_steps=2048, policy_kwargs={'features_extractor_class': CustomCnnPolicy})
model.learn(total_timesteps=100000, callback=callback)

# Testing the Model
env = VizDoomGym(config='github/VizDoom/scenarios/basic.cfg', render=True)
mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)
print(f'Mean reward: {mean_reward}')
for episode in range(10):
    obs = env.reset()
    done = False
    total_reward = 0
    while not done:
        action, _ = model.predict(obs)
        obs, reward, done, info = env.step(action)
        total_reward += reward
    print(f'Total Reward for episode {episode}: {total_reward}')
env.close()
